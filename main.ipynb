{"cells":[{"cell_type":"markdown","metadata":{"id":"XCKqzsF9IwlF"},"source":["# Deep Learning for the Auto-Generated Music Composition\n","\n","Name: Jianqiao Li, Zhiying Cui\n","\n","NetID: jl7136, zc2191"]},{"cell_type":"markdown","metadata":{"id":"malRdCGoIwlI"},"source":["## Introduction\n","\n","- This project is based on the DeepJ model from the github repository [DeepJ: A model for style-specific music generation](https://github.com/calclavia/DeepJ) with some modifications.\n","- The reference paper is [Mao HH, Shin T, Cottrell G. DeepJ: Style-specific music generation. In2018 IEEE 12th International Conference on Semantic Computing (ICSC) 2018 Jan 31 (pp. 377-382). IEEE.](https://arxiv.org/abs/1801.00887)."]},{"cell_type":"markdown","metadata":{"id":"9Qs3_FCjIwlJ"},"source":["## Goal of This Work\n","\n","Our model aims to auto-generate a 10 to 30 seconds polyphony given a specific music style and random formatted inputs. Objectives are as follows:\n","- Rebuild the DeepJ model on our local environment and compare the results with the authors’ model.\n","- Replace the one-hot encoding strategy on style generation with a pre-trained model.\n","- Try to develop a sparse representation of music as the authors recommended."]},{"cell_type":"markdown","metadata":{"id":"SVJUbqOuIwlJ"},"source":["## Development Environment\n","\n","- Python version: Python 3.6.\n","    - Considering the incompatible version of `grpcio` in Python 3.5 for TensorFlow 2, we decide to use Python 3.6 which is different from the DeepJ repo.\n","- Framework: TensorFlow.\n","- Environment: Google Cloud.\n","\n","### Some useful tips for setting up\n","\n","- Set up a jupyter server on Google Cloud: \n","    - [Running Jupyter Notebook on Google Cloud Platform in 15 min](https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52).\n","- Add a different version of kernel:\n","    - [How to add python 3.6 kernel alongside 3.5 on jupyter](https://stackoverflow.com/questions/43759610/how-to-add-python-3-6-kernel-alongside-3-5-on-jupyter).\n","    - [Jupyter Notebook Kernels: How to Add, Change, Remove](https://queirozf.com/entries/jupyter-kernels-how-to-add-change-remove).\n","    - [Run Jupyter Notebook script from terminal](https://deeplearning.lipingyang.org/2018/03/28/run-jupyter-notebook-script-from-terminal/).\n","    \n","### Requirements\n","\n","- Install dependencies for the DeepJ.\n","```\n","pip install --ignore-installed -r requirements.txt\n","```\n","- Install `python-midi` module. The original [python-midi](https://github.com/vishnubob/python-midi) is no longer maintained. We have to find an alternative python-midi from the following repos:\n","    - ✅ Candidate 1: https://github.com/louisabraham/python3-midi\n","    - ❓ Candidate 2: https://github.com/sniperwrb/python-midi\n","    - ❓ Candidate 3: https://github.com/jameswenzel/mydy\n","- Download the dataset to `data` folder. The Midi files come from [Piano-midi](http://www.piano-midi.de/).\n","- Details about the project directory:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WSMeOCSIwlK","outputId":"fd384d43-bc61-4f26-a861-1c97ee6dedfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Project Directory\n","\u001b[01;34m.\u001b[00m\r\n","├── LICENSE\r\n","├── README.md\r\n","├── \u001b[01;34m__pycache__\u001b[00m\r\n","├── \u001b[01;34marchives\u001b[00m\r\n","├── constants.py\r\n","├── \u001b[01;34mdata\u001b[00m\r\n","├── dataset.py\r\n","├── distribution.py\r\n","├── download.py\r\n","├── generate.py\r\n","├── \u001b[01;34mimages\u001b[00m\r\n","├── main.ipynb\r\n","├── midi_util.py\r\n","├── model.py\r\n","├── nohup.out\r\n","├── \u001b[01;34mout\u001b[00m\r\n","├── requirements.txt\r\n","├── \u001b[01;34mscripts\u001b[00m\r\n","├── test.py\r\n","├── train.py\r\n","├── util.py\r\n","└── visualize.py\r\n","\r\n","6 directories, 16 files\r\n"]}],"source":["import os\n","\n","print('Project Directory')\n","os.chdir('/home/choi/DLFinalProject/')\n","!tree -L 1"]},{"cell_type":"markdown","metadata":{"id":"_KfnK92RIwlL"},"source":["Before getting start, import all dependencies and modules required for this work."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9b4UXeWIwlM","outputId":"59bcfd4f-caf6-49d5-f34a-57069d40f712"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow version: 2.6.2\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","from keras.callbacks import EarlyStopping, TensorBoard\n","\n","from constants import *             # store constant parameters for the model\n","from dataset import *               # load dataset and parse to formated inputs\n","from generate import *              # generate music\n","from model import *                 # model architectures\n","from midi_util import midi_encode   # util funcs for midi\n","from util import *                  # util funcs\n","\n","print(\"TensorFlow version:\", tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"Ogf0iQtSIwlM"},"source":["## Objective 1: Rebuild the DeepJ Model\n","\n","### Dataset\n","\n","We adopted the same dataset as the authors used in training DeepJ. [Piano-midi](http://www.piano-midi.de/midicoll.htm) contains a dataset of classical piano solo pieces. The pieces of each composer are recorded by using a Midi sequencer. There are 571 pieces composed by 26 composers with a total duration of 36.7 hours of MIDI files in this dataset till Feb. 2020. There are also some alternative datasets such as [mfiles](https://www.mfiles.co.uk/).\n","\n","Considering the computing power of our Google Cloud server, we eliminated one genre \"baroque\" and reduce the number of composers to 6. Details can be found in `constant.py` and all Midi data is saved in `data` folder. Before focusing on the model, we first need to download the dataset and parse them into formatted inputs. All utility functions to process the Midi file are coded in `dataset.py`. Several functions need to be paid attention to:\n","\n","- `load_all(styles, batch_size, time_steps)`: Load all Midi files and parse them into four inputs, that are `note_data`, `note_target`, `beat_data`, `style_data`, and one label `note_target`.\n","- `clamp_midi(sequence)`: Clamp the Midi based on the `MIN` and `MAX` notes. In the paper, the authors truncate a standard pitch to range from 36 to 84 to reduce the input dimension. \n","- `stagger(data, time_steps)`: Chop the sequence data by `time_steps`. This function returns two variables: `dataX` the sequence of data in the current time step, and `dataY` the sequence of data in the next time step which is the predicted target.\n","\n","Now, load all Midi files in `data/` folder and see how each input data looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATUTL-fZIwlN"},"outputs":[],"source":["# load data\n","train_data, train_labels = load_all(styles, BATCH_SIZE, SEQ_LEN) # actually we can safely remove BATCH_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7z0xWzAIwlN","outputId":"7c31b2cc-d87d-487f-ca14-611f161f82e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["note_data: (2306, 128, 48, 3)\n","note_target: (2306, 128, 48, 3)\n","beat_data: (2306, 128, 16)\n","style_data: (2306, 128, 6)\n"]}],"source":["print(\"note_data:\", train_data[0].shape)\n","print(\"note_target:\", train_data[1].shape) # aka train_labels\n","print(\"beat_data:\", train_data[2].shape)\n","print(\"style_data:\", train_data[3].shape)"]},{"cell_type":"markdown","metadata":{"id":"JYGrEZnOIwlO"},"source":["Noted that all constant parameters for the model are saved in `constants.py` file. The following table gives the meaning of each variable. (Here we use syntax of Java to present the constant type).\n","\n","| Variable          | Value/Type        | Representation            |\n","| :---------------- | :---------------: | :------------------------ |\n","| genre             | List<String>      | Genre of music            |\n","| styles            | List<List<String>>| Directory of dataset      |\n","| NUM_STYLES        | styles.size()     | Numbers of styles         |\n","| DEFAULT_RES       | 96                | Resolution                |\n","| MIDI_MAX_NOTES    | 128               | Notes range [1, 128]      |\n","| MAX_VELOCITY      | 127               | Velocity range [0, 127]   |\n","| NUM_OCTAVES       | 4                 | Number of octaves         |\n","| OCTAVE            | 12                | Notes in every octave     |\n","| MIN_NOTE          | 36                | Minimum note              |\n","| MAX_NOTE          | MIN_NOTE + NUM_OCTAVES * OCTAVE       | Maximum note                  |\n","| NUM_NOTES         | MAX_NOTE - MIN_NOTE   | Number of notes between MIN_NOTE and MAX_NOTE |\n","| BEATS_PER_BAR     | 4                 | Number of beats in a bar  |\n","| NOTES_PER_BEAT    | 4                 | Notes per quarter note    |\n","| NOTES_PER_BAR     | NOTES_PER_BEAT * BEATS_PER_BAR    | The quickest note is a half-note  |\n","| BATCH_SIZE        | 16                | Training batch size       |\n","| SEQ_LEN           | 8 * NOTES_PER_BAR | Data sequence length      |\n","| OCTAVE_UNITS      | 64                | Dim of hyperparameter in octave convolution layer |\n","| STYLE_UNITS       | 64                | Dim of hyperparameter in style embedding          |\n","| NOTE_UNITS        | 3                 | Three outputs: play prob, replay prob and dynamics|\n","| TIME_AXIS_UNITS   | 256               | Dim of hyperparameter used for LSTMs in Time-Axis |\n","| NOTE_AXIS_UNITS   | 128               | Dim of hyperparameter used for LSTMs in Note-Axis |\n"]},{"cell_type":"markdown","metadata":{"id":"Z26DSWE1IwlO"},"source":["### Model architecture\n","\n","The DeepJ architecture is the following.\n","\n","![DeepJ](./images/architecture.png)\n","\n","The corresponding codes are written in `model.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"FhWyQwxHIwlO"},"outputs":[],"source":["def build_models(time_steps=SEQ_LEN, input_dropout=0.2, dropout=0.5):\n","    \"\"\"\n","    Build the LSTM model\n","    \"\"\"\n","    notes_in = Input((time_steps, NUM_NOTES, NOTE_UNITS))   # Note input\n","    beat_in = Input((time_steps, NOTES_PER_BAR))            # Context\n","    style_in = Input((time_steps, NUM_STYLES))              # Style\n","    # Target input for conditioning, feed-forward\n","    chosen_in = Input((time_steps, NUM_NOTES, NOTE_UNITS))  # Chosen notes\n","\n","    # Dropout inputs\n","    notes = Dropout(input_dropout)(notes_in)\n","    beat = Dropout(input_dropout)(beat_in)\n","    chosen = Dropout(input_dropout)(chosen_in)\n","\n","    # Distributed representations\n","    style_l = Dense(STYLE_UNITS, name='style')\n","    style = style_l(style_in)\n","\n","    \"\"\" Time axis \"\"\"\n","    time_out = time_axis(dropout)(notes, beat, style)\n","\n","    \"\"\" Note Axis \"\"\"\n","    naxis = note_axis(dropout)              # 1D Convolution\n","\n","    \"\"\" Prediction Layer \"\"\"\n","    notes_out = naxis(time_out, chosen, style)\n","\n","    \"\"\" Build Model \"\"\"\n","    model = Model([notes_in, chosen_in, beat_in, style_in], [notes_out])\n","    model.compile(optimizer='nadam',        # Nesterov Adam optimizer\n","                  loss=[primary_loss])      # Loss function\n","\n","    \"\"\" Generation Models \"\"\"\n","    time_model = Model([notes_in, beat_in, style_in], [time_out])\n","\n","    note_features = Input((1, NUM_NOTES, TIME_AXIS_UNITS), name='note_features')\n","    chosen_gen_in = Input((1, NUM_NOTES, NOTE_UNITS), name='chosen_gen_in')\n","    style_gen_in = Input((1, NUM_STYLES), name='style_in')\n","\n","    # Dropout inputs\n","    chosen_gen = Dropout(input_dropout)(chosen_gen_in)\n","    style_gen = style_l(style_gen_in)\n","\n","    note_gen_out = naxis(note_features, chosen_gen, style_gen)\n","    note_model = Model([note_features, chosen_gen_in, style_gen_in], note_gen_out)\n","\n","    return model, time_model, note_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6d043ADIwlO","outputId":"49af1b5b-2128-4d89-982d-a11833d4955e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 128, 48, 3)] 0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 128, 6)]     0                                            \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 128, 48, 3)   0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","style (Dense)                   multiple             448         input_3[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 128, 48, 64)  4672        dropout[0][0]                    \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 128, 16)]    0                                            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128, 94)      6110        style[0][0]                      \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 128, 48, 64)  0           time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 128, 16)      0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 128, 48, 94)  0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 128, 48, 1)   0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 128, 48, 12)  0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 128, 48, 1)   0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 128, 48, 64)  0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 128, 48, 16)  0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 128, 48, 94)  0           time_distributed_2[0][0]         \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 128, 48, 94)  0           lambda[0][0]                     \n","                                                                 lambda_1[0][0]                   \n","                                                                 lambda_2[0][0]                   \n","                                                                 dropout_3[0][0]                  \n","                                                                 time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128, 48, 94)  0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128, 256)     16640       style[0][0]                      \n","__________________________________________________________________________________________________\n","permute (Permute)               (None, 48, 128, 94)  0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","permute_1 (Permute)             (None, 48, 128, 94)  0           dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, 128, 48, 256) 0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 48, 128, 94)  0           permute[0][0]                    \n","                                                                 permute_1[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 128, 48, 256) 0           time_distributed_4[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 48, 128, 256) 359424      add[0][0]                        \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 128, 48, 256) 0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 48, 128, 256) 0           time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","permute_2 (Permute)             (None, 48, 128, 256) 0           dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 48, 128, 256) 0           dropout_5[0][0]                  \n","                                                                 permute_2[0][0]                  \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 128, 48, 3)] 0                                            \n","__________________________________________________________________________________________________\n","time_distributed_5 (TimeDistrib (None, 48, 128, 256) 525312      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 128, 48, 3)   0           input_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 multiple             16835       style[0][0]                      \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 48, 128, 256) 0           time_distributed_5[0][0]         \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 128, 48, 3)   0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_6 (TimeDistrib (None, 128, 48, 259) 0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","permute_3 (Permute)             (None, 128, 48, 256) 0           dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 128, 48, 3)   0           lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 128, 48, 259) 0           time_distributed_6[0][0]         \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 128, 48, 259) 0           permute_3[0][0]                  \n","                                                                 reshape[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 128, 48, 259) 0           activation_3[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 multiple             8320        style[0][0]                      \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 128, 48, 259) 0           concatenate_1[0][0]              \n","                                                                 dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_8 (TimeDistrib (None, 128, 48, 128) 0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_7 (TimeDistrib (None, 128, 48, 128) 198656      add_2[0][0]                      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 128, 48, 128) 0           time_distributed_8[0][0]         \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 128, 48, 128) 0           time_distributed_7[0][0]         \n","__________________________________________________________________________________________________\n","dropout_10 (Dropout)            (None, 128, 48, 128) 0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 128, 48, 128) 0           dropout_9[0][0]                  \n","                                                                 dropout_10[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_9 (TimeDistrib (None, 128, 48, 128) 131584      add_3[0][0]                      \n","__________________________________________________________________________________________________\n","dropout_11 (Dropout)            (None, 128, 48, 128) 0           time_distributed_9[0][0]         \n","__________________________________________________________________________________________________\n","note_dense (Dense)              multiple             258         dropout_11[0][0]                 \n","__________________________________________________________________________________________________\n","volume_dense (Dense)            multiple             129         dropout_11[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 128, 48, 3)   0           note_dense[0][0]                 \n","                                                                 volume_dense[0][0]               \n","==================================================================================================\n","Total params: 1,268,388\n","Trainable params: 1,268,388\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["models = build_models()\n","models[0].summary() # LSTM model: params 1,268,388"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMlWet0tIwlP","outputId":"298184b5-e5a9-4ae1-fd13-88251a9eee84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 128, 48, 3)] 0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 128, 6)]     0                                            \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 128, 48, 3)   0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","style (Dense)                   multiple             448         input_3[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 128, 48, 64)  4672        dropout[0][0]                    \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 128, 16)]    0                                            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128, 94)      6110        style[0][0]                      \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 128, 48, 64)  0           time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 128, 16)      0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 128, 48, 94)  0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 128, 48, 1)   0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 128, 48, 12)  0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 128, 48, 1)   0           dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 128, 48, 64)  0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 128, 48, 16)  0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 128, 48, 94)  0           time_distributed_2[0][0]         \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 128, 48, 94)  0           lambda[0][0]                     \n","                                                                 lambda_1[0][0]                   \n","                                                                 lambda_2[0][0]                   \n","                                                                 dropout_3[0][0]                  \n","                                                                 time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128, 48, 94)  0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128, 256)     16640       style[0][0]                      \n","__________________________________________________________________________________________________\n","permute (Permute)               (None, 48, 128, 94)  0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","permute_1 (Permute)             (None, 48, 128, 94)  0           dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, 128, 48, 256) 0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 48, 128, 94)  0           permute[0][0]                    \n","                                                                 permute_1[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 128, 48, 256) 0           time_distributed_4[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 48, 128, 256) 359424      add[0][0]                        \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 128, 48, 256) 0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 48, 128, 256) 0           time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","permute_2 (Permute)             (None, 48, 128, 256) 0           dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 48, 128, 256) 0           dropout_5[0][0]                  \n","                                                                 permute_2[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_5 (TimeDistrib (None, 48, 128, 256) 525312      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 48, 128, 256) 0           time_distributed_5[0][0]         \n","__________________________________________________________________________________________________\n","permute_3 (Permute)             (None, 128, 48, 256) 0           dropout_7[0][0]                  \n","==================================================================================================\n","Total params: 912,606\n","Trainable params: 912,606\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","style_in (InputLayer)           [(None, 1, 6)]       0                                            \n","__________________________________________________________________________________________________\n","chosen_gen_in (InputLayer)      [(None, 1, 48, 3)]   0                                            \n","__________________________________________________________________________________________________\n","style (Dense)                   multiple             448         style_in[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_12 (Dropout)            (None, 1, 48, 3)     0           chosen_gen_in[0][0]              \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 multiple             16835       style[1][0]                      \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 1, 48, 3)     0           dropout_12[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_10 (TimeDistri (None, 1, 48, 259)   0           dense_2[1][0]                    \n","__________________________________________________________________________________________________\n","note_features (InputLayer)      [(None, 1, 48, 256)] 0                                            \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 1, 48, 3)     0           lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 1, 48, 259)   0           time_distributed_10[0][0]        \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 1, 48, 259)   0           note_features[0][0]              \n","                                                                 reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_13 (Dropout)            (None, 1, 48, 259)   0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 multiple             8320        style[1][0]                      \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 1, 48, 259)   0           concatenate_3[0][0]              \n","                                                                 dropout_13[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_12 (TimeDistri (None, 1, 48, 128)   0           dense_3[1][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_11 (TimeDistri (None, 1, 48, 128)   198656      add_4[0][0]                      \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 1, 48, 128)   0           time_distributed_12[0][0]        \n","__________________________________________________________________________________________________\n","dropout_14 (Dropout)            (None, 1, 48, 128)   0           time_distributed_11[0][0]        \n","__________________________________________________________________________________________________\n","dropout_15 (Dropout)            (None, 1, 48, 128)   0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 1, 48, 128)   0           dropout_14[0][0]                 \n","                                                                 dropout_15[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_13 (TimeDistri (None, 1, 48, 128)   131584      add_5[0][0]                      \n","__________________________________________________________________________________________________\n","dropout_16 (Dropout)            (None, 1, 48, 128)   0           time_distributed_13[0][0]        \n","__________________________________________________________________________________________________\n","note_dense (Dense)              multiple             258         dropout_16[0][0]                 \n","__________________________________________________________________________________________________\n","volume_dense (Dense)            multiple             129         dropout_16[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 1, 48, 3)     0           note_dense[1][0]                 \n","                                                                 volume_dense[1][0]               \n","==================================================================================================\n","Total params: 356,230\n","Trainable params: 356,230\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["models[1].summary() # Time axis: params 912,606\n","models[2].summary() # Note axis: params 356,230"]},{"cell_type":"markdown","metadata":{"id":"f-qrGCGTIwlP"},"source":["### Training\n","\n","Training was performed using stochastic gradient descent with the Nesterov Adam optimizer. The loss function is as follows:\n","\n","$$\n","\\begin{equation*}\n","    \\begin{split}\n","        & L_{play} = \\sum {t_{play}\\log y_{play} + (1 - t_{play}) log(1-y_{play})}\\\\\n","        & L_{rplay} = \\sum {t_{play}(t_{rplay}\\log y_{rplay} + (1 - t_{rplay}) log(1 - y_{rplay}))}\\\\\n","        & L_{dynamics} = \\sum {t_{play}(t_{dynamics} - y_{dynamics})^2} \\\\\n","        & L_{primary} = L_{play} + L_{rplay} + L_{dynamics}\n","    \\end{split}\n","\\end{equation*}\n","$$\n","\n","Play and replay are treated as logistic regression problems trained using binary cross entropy, as defined in a Biaxial LSTM. Dynamics(velocity) is trained using mean squared error. Training module is in file `train.py`.\n","\n","Noted that we decrease the `patience` parameter in `EarlyStopping` to 3, and reduce the number of `epochs` to 100. In our preliminary experiment, we find that it commonly spends ~800s for training one epoch on our server. And it requires the epochs ~120 to get the relatively optimal model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYPJXZx-IwlP"},"outputs":[],"source":["def train(models):\n","    cbs = [\n","        ModelCheckpoint(MODEL_FILE, monitor='loss', save_best_only=True, save_weights_only=True),\n","        EarlyStopping(monitor='loss', patience=3),\n","        TensorBoard(log_dir='out/logs', histogram_freq=1)\n","    ]\n","\n","    print('Training')\n","    models[0].fit(train_data, train_labels, epochs=200, callbacks=cbs, batch_size=BATCH_SIZE)\n","\n","train(models)"]},{"cell_type":"markdown","metadata":{"id":"zdAiPduQIwlP"},"source":["### Generation\n","\n","After having the trained model, we need to auto generate the music. Authors performed generation by sampling from the model’s probability distribution using a coin flip to determine whether to play a note or not. After deciding to play a note, they sample from the replay probability to determine if the note should be re-attacked. Dynamics level is directly used from the model given that the note is played.\n","\n","In our work, we decide to use a different method to generate music. We are going to provide a piece of Midi file cut from the training dataset and observe how does the model work. Further, in DeepJ model, authors use an adaptive temperature adjustment to avoid long period of silence, which is a tricky and smart method we adopt the same. All util functions related to music generation are in `generate.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSnAW2WyIwlQ"},"outputs":[],"source":["def generateModified(models, num_bars, styles, start_notes):\n","    print('Generating with styles:', styles)\n","\n","    _, time_model, note_model = models\n","    generations = [MusicGeneration(style) for style in styles]\n","\n","    for t in tqdm(range(NOTES_PER_BAR * num_bars)):\n","        # Produce note-invariant features\n","        ins = process_inputs([g.build_time_inputs() for g in generations])\n","        \n","        # Use starts notes\n","        ins[0][0] = start_notes[t]\n","        ins[0][1] = start_notes[t]\n","\n","        # Pick only the last time step\n","        note_features = time_model.predict(ins)\n","        note_features = np.array(note_features)[:, -1:, :]\n","\n","        # Generate each note conditioned on previous\n","        for n in range(NUM_NOTES):\n","            ins = process_inputs([g.build_note_inputs(note_features[i, :, :, :]) for i, g in enumerate(generations)])\n","            predictions = np.array(note_model.predict(ins))\n","\n","            for i, g in enumerate(generations):\n","                # Remove the temporal dimension\n","                g.choose(predictions[i][-1], n)\n","\n","        # Move one time step\n","        yield [g.end_time(t) for g in generations]\n","\n","# generate music\n","print('Load model from file.')\n","models[0].load_weights(MODEL_FILE)\n","\n","stylesGene = [compute_genre(i) for i in range(len(genre))]\n","\n","write_file('output', generateModified(models, 32, stylesGene, train_data[0]))"]},{"cell_type":"markdown","metadata":{"id":"jEcdxuksIwlQ"},"source":["## Objective 2: Music Genre Classification\n","\n","As we can see above, the DeepJ model actually generates music by music genre rather than one specific composer. It mixes all composers' composition styles under the same genre into one-hot encoding. From the perspective of the dataset, it seems insignificant to train the model using the dataset classified by composers. \n","\n","Besides, the music genres such as Baroque, Classicism, and Romanticism are known as the different periods of ages. It is hard for a human with basic music knowledge to distinguish which period a given piece belongs to. But for a deep learning model, it is probably able to distinguish the certain pattern behind the notes and beats.\n","\n","Our next step is to pre-train a music genre classification to replace the style encoding `style_in` in the DeepJ model. When we start this part of work, our server is suspended by Google because of suspicious coin mining activity. So, we decide to continue our work on the Colab.\n","\n","There are some excellent models for music genre classification models by using CNN, which identify spectrograms of various music genres, such as references [Music Genres Classification using Deep learning techniques](https://www.analyticsvidhya.com/blog/2021/06/music-genres-classification-using-deep-learning-techniques/), [Music Genre Classification](http://cs229.stanford.edu/proj2018/report/21.pdf). Since our dataset is represented by notes and beats matrixes. We are going to build an SVM music genre classification model. In the work by [Chet N. Gnegy](http://cs229.stanford.edu/proj2014/Chet%20Gnegy,Classification%20Of%20Musical%20Playing%20Styles.pdf), the author concluded that by using the right feature extraction, the accuracy of the SVM genre classification model can exceed 90%. In our music genre classification, we adopt the same feature extraction method in this repo [Midi Classification Tutorial](https://github.com/sandershihacker/midi-classification-tutorial).\n","\n","For the style encoding `style_in` in our model, we literally want to use it to represent what a piece of music sounds like by a human. Therefore, the style encoding is supposed to be a vector with the value of the probability of the music genre. However, the SVM models don’t output probabilities natively. We have to convert the output to class probabilities. Among all possible approaches, the Platt scaling is particularly suitable for SVMs, referenced from [Can you interpret probabilistically the output of a Support Vector Machine?](https://mmuratarat.github.io/2019-10-12/probabilistic-output-of-svm).\n"]},{"cell_type":"markdown","metadata":{"id":"FPNP8gRyYFzN"},"source":["### Import modules"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4344,"status":"ok","timestamp":1639945680948,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"oK_6vbHwKSO0","outputId":"e052a849-5fb8-4e81-8f0e-0cba3cd226a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pretty_midi in /usr/local/lib/python3.7/dist-packages (0.2.9)\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n","Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n"]}],"source":["!pip install pretty_midi"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":126,"status":"ok","timestamp":1639953128583,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"cSJDhvQQJ62N"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","from sklearn.calibration import CalibratedClassifierCV\n","import pretty_midi\n","import warnings\n","import os"]},{"cell_type":"markdown","metadata":{"id":"eBMv8ks5YKgI"},"source":["Mount the google drive. The working directory in the google drive is located at `/content/drive/MyDrive/DLFinalProject`."]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":703,"status":"ok","timestamp":1639945681643,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"caHEA7qjYJay","outputId":"7a88f6c9-de60-4d38-e29a-336eb35c391e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/DLFinalProject\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# change to the working dirctory\n","%cd /content/drive/MyDrive/DLFinalProject"]},{"cell_type":"markdown","metadata":{"id":"gXPW7BBwOihQ"},"source":["### Download and parse the dataset"]},{"cell_type":"markdown","metadata":{"id":"4Ki0e52Sg4kC"},"source":["Here, we are going to use another dataset to evaluate the performance of the music genre classification. This dataset contains more genres and more durations of music pieces than the piano-midi dataset, which is better to visualize how this model works."]},{"cell_type":"markdown","metadata":{"id":"Mk9z22l7Ozyf"},"source":["#### Download dataset\n","\n","- Genre labels: [tagtraum industries](http://www.tagtraum.com/msd_genre_datasets.html) -> Genre Ground Truth -> \"CD1\"\n","- Midi files: [The Lakh MIDI Dataset](http://colinraffel.com/projects/lmd/) -> LMD-matched"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1432,"status":"ok","timestamp":1639926424316,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"dkPjeRQbKWgy","outputId":"362f7f5a-f4e7-4c9c-f564-3fc8848fd072"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2021-12-19 15:07:02--  https://www.tagtraum.com/genres/msd_tagtraum_cd1.cls.zip\n","Resolving www.tagtraum.com (www.tagtraum.com)... 81.169.145.77, 2a01:238:20a:202:1077::\n","Connecting to www.tagtraum.com (www.tagtraum.com)|81.169.145.77|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1183001 (1.1M) [application/zip]\n","Saving to: ‘msd_tagtraum_cd1.cls.zip’\n","\n","msd_tagtraum_cd1.cl 100%[===================>]   1.13M  1.71MB/s    in 0.7s    \n","\n","2021-12-19 15:07:04 (1.71 MB/s) - ‘msd_tagtraum_cd1.cls.zip’ saved [1183001/1183001]\n","\n"]}],"source":["# genre labels\n","!wget https://www.tagtraum.com/genres/msd_tagtraum_cd1.cls.zip\n","!unzip msd_tagtraum_cd1.cls.zip"]},{"cell_type":"markdown","metadata":{"id":"RMxo4AHHQTS5"},"source":["Map the genre to index."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1639946649739,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"yTDBUs5mQb_a","outputId":"6a41f7e7-803d-4a66-cca8-9f2bd206219f"},"outputs":[{"name":"stdout","output_type":"stream","text":["      Genre             TrackID\n","0  Pop_Rock  TRAAAAK128F9318786\n","1       Rap  TRAAAAW128F429D538\n","2  Pop_Rock  TRAAABD128F429CF47\n","3      Jazz  TRAAAED128E0783FAB\n","4  Pop_Rock  TRAAAEF128F4273421 \n","\n","['Pop_Rock', 'Country', 'International', 'Jazz', 'Electronic', 'Rap', 'Reggae', 'RnB', 'Latin', 'Blues', 'New Age', 'Vocal', 'Folk'] \n","\n","{'Pop_Rock': 0, 'Country': 1, 'International': 2, 'Jazz': 3, 'Electronic': 4, 'Rap': 5, 'Reggae': 6, 'RnB': 7, 'Latin': 8, 'Blues': 9, 'New Age': 10, 'Vocal': 11, 'Folk': 12} \n","\n"]}],"source":["def get_genres(path):\n","    ids = []\n","    genres = []\n","    with open(path) as f:\n","        line = f.readline()\n","        while line:\n","            if line[0] != '#':\n","                [x, y, *_] = line.strip().split(\"\\t\")\n","                ids.append(x)\n","                genres.append(y)\n","            line = f.readline()\n","    genre_df = pd.DataFrame(data={\"Genre\": genres, \"TrackID\": ids})\n","    return genre_df\n","\n","# get the genre dataFrame\n","genre_path = \"msd_tagtraum_cd1.cls\"\n","genre_df = get_genres(genre_path)\n","\n","# mapping\n","label_list = list(set(genre_df.Genre))\n","label_dict = {lbl: label_list.index(lbl) for lbl in label_list}\n","\n","print(label_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tocuuobGPhsm"},"outputs":[],"source":["# midi files\n","!wget http://hog.ee.columbia.edu/craffel/lmd/lmd_matched.tar.gz\n","!tar -xzvf lmd_matched.tar.gz -C /content/drive/MyDrive/DLFinalProject/"]},{"cell_type":"markdown","metadata":{"id":"E63XZ2DWSa8s"},"source":["Map midi filepath to genre."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33304,"status":"ok","timestamp":1639946686954,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"VvYIIZFHSe0_","outputId":"f02a0093-f7a4-40af-a9e9-b0b27161bd5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                Path          Genre\n","0  lmd_matched/L/L/M/TRLLMMQ128F423119C/9cafb699c...       Pop_Rock\n","1  lmd_matched/L/L/Q/TRLLQXV128E07943FB/fe9fee992...       Pop_Rock\n","2  lmd_matched/L/L/R/TRLLRHS128E079431A/0769d0162...       Pop_Rock\n","3  lmd_matched/L/L/Y/TRLLYMC128F146EB42/c3ad21f1c...  International\n","4  lmd_matched/L/L/Y/TRLLYMC128F146EB42/198c242da...  International\n","(13360, 2)\n"]}],"source":["def get_matched_midi(midi_folder, genre_df):\n","    track_ids, file_paths = [], []\n","    for dir_name, subdir_list, file_list in os.walk(midi_folder):\n","        if len(dir_name) == 36:\n","            track_id = dir_name[18:]\n","            file_path_list = [\"/\".join([dir_name, file]) for file in file_list]\n","            for file_path in file_path_list:\n","                track_ids.append(track_id)\n","                file_paths.append(file_path)\n","    all_midi_df = pd.DataFrame({\"TrackID\": track_ids, \"Path\": file_paths})\n","    # join with genre\n","    df = pd.merge(all_midi_df, genre_df, on='TrackID', how='inner')\n","    return df.drop([\"TrackID\"], axis=1)\n","\n","# mapping\n","midi_path = \"lmd_matched\"\n","matched_midi_df = get_matched_midi(midi_path, genre_df)\n","\n","print(matched_midi_df.head())\n","print(matched_midi_df.shape)"]},{"cell_type":"markdown","metadata":{"id":"EW4BcL1EePo5"},"source":["#### Extract features\n","\n","The size of total midi files is so large that we do not have enough RAM to train the model on the colab. Therefore, we have to cut down the number and randomly select 1000 samples for our music genere classification model."]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1639946686955,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"bH6FB_GOcI-g","outputId":"fcfe7cd7-59ea-42b9-9297-d06148b3b6de"},"outputs":[{"name":"stdout","output_type":"stream","text":["1000\n","(1000, 2)\n"]}],"source":["def getRandomIndex(n, x):\n","    index = np.random.choice(np.arange(n), size=x, replace=False)\n","    return index\n","\n","data_index = getRandomIndex(len(matched_midi_df), 1000)\n","matched_midi = pd.DataFrame(matched_midi_df, index=data_index)\n","print(matched_midi.shape)"]},{"cell_type":"markdown","metadata":{"id":"pJsYjimmfs4R"},"source":["The next step is to extract appropriate features from the midi file. Differ from the DeepJ model, in the music genre classification we need to decide to use another set of features that are more likely contributing to classification. Taking the advantages of python packages `python-midi`, we decide to use similar features as the author in [Midi Classification Tutorial](https://github.com/sandershihacker/midi-classification-tutorial). He has proved this set of features is able to achieve more than 70% precision. Features are as follows, details for each feature please refer to [pretty-midi](https://craffel.github.io/pretty-midi/):\n","- Tempo\n","- Number of chord signature changes\n","- Resolution\n","- Time signature"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3121914,"status":"ok","timestamp":1639949808854,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"7UyaqheueAmA","outputId":"15d9ea67-a70c-44fb-b77e-2ba7020bd97e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.29766837  0.55        0.125       0.125       0.        ]\n"," [ 0.31238361  0.55        0.125       0.125       0.        ]\n"," [ 0.0283018  -0.35        0.125       0.125       2.        ]\n"," ...\n"," [ 0.04577074  0.31        0.125       0.125       0.        ]\n"," [ 0.17373668 -0.35        0.125       0.125       0.        ]\n"," [ 0.3973189   1.75        0.125       0.125       0.        ]]\n","CPU times: user 50min 37s, sys: 3min 23s, total: 54min 1s\n","Wall time: 52min 1s\n"]}],"source":["%%time\n","def normalize_features(features):\n","    \"\"\"\n","    range [-1, 1]\n","    \"\"\"\n","    tempo = (features[0] - 150) / 300\n","    num_sig_changes = (features[1] - 2) / 10\n","    resolution = (features[2] - 260) / 400\n","    time_sig_1 = (features[3] - 3) / 8\n","    time_sig_2 = (features[4] - 3) / 8\n","    return [tempo, resolution, time_sig_1, time_sig_2]\n","\n","\n","def get_features(path):\n","    try:\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"error\")\n","            file = pretty_midi.PrettyMIDI(path)\n","            \n","            tempo = file.estimate_tempo()\n","            num_sig_changes = len(file.time_signature_changes)\n","            resolution = file.resolution\n","            ts_changes = file.time_signature_changes\n","            ts_1 = 4\n","            ts_2 = 4\n","            if len(ts_changes) > 0:\n","                ts_1 = ts_changes[0].numerator\n","                ts_2 = ts_changes[0].denominator\n","            return normalize_features([tempo, num_sig_changes, resolution, ts_1, ts_2])\n","    except:\n","        return None\n","\n","\n","def extract_midi_features(path_df):\n","    all_features = []\n","    for index, row in path_df.iterrows():\n","        features = get_features(row.Path)   # [tempo, num_sig_changes, resolution, ts_1, ts_2]\n","        genre = label_dict[row.Genre]       # [label]\n","        if features is not None:\n","            features.append(genre)\n","            all_features.append(features)\n","    return np.array(all_features)\n","\n","labeled_features = extract_midi_features(matched_midi_df) # [tempo, num_sig_changes, resolution, ts_1, ts_2, label]\n","print(labeled_features)"]},{"cell_type":"markdown","metadata":{"id":"sz5biCc5mcK-"},"source":["The overall feature extraction procedure takes about one hours."]},{"cell_type":"markdown","metadata":{"id":"hTmrfrzwkbcb"},"source":["#### Partition dataset\n","\n","We will split the dataset into training datasets: validation dataset: test dataset = 6:2:2, and partition them to `x` and `y`, where `x` is the features from `labeled_features` and `y` is the music genre label.\n","\n","Since the whole dataset is selected by using random indexes, there is no need to shuffle the dataset again."]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116,"status":"ok","timestamp":1639951762236,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"9twXoxdLkkr1","outputId":"556c4b57-05e0-48e7-d0b5-a8e0bb020bdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.23680891  0.31        0.125       0.125       0.        ]\n"," [ 0.25503977  0.31        0.125       0.125       0.        ]\n"," [ 0.09117949 -0.35        0.125       0.125      11.        ]\n"," [ 0.11965721 -0.17        0.125       0.125       0.        ]\n"," [ 0.18804874 -0.17        0.125       0.125       0.        ]\n"," [ 0.20735881 -0.17        0.125       0.125       0.        ]\n"," [ 0.23354232 -0.575       0.125       0.125       0.        ]\n"," [ 0.1413318  -0.17        0.125       0.125       0.        ]\n"," [ 0.14782987  0.55        0.125       0.125       0.        ]\n"," [ 0.14782987  0.55        0.125       0.125       0.        ]]\n","[ 0  0 11  0  0  0  0  0  0  0]\n"]}],"source":["# split the dataset: train_labeled_features, valid_labeled_features, test_labeled_features\n","num = len(labeled_features)\n","num_train = int(num * 0.6)\n","num_valid = int(num * 0.8)\n","train_labeled_features = labeled_features[:num_train]\n","valid_labeled_features = labeled_features[num_train:num_valid]\n","test_labeled_features = labeled_features[num_valid:]\n","\n","# format to x and y\n","cols = train_labeled_features.shape[1] - 1\n","x_train = train_labeled_features[:, :cols]\n","x_valid = valid_labeled_features[:, :cols]\n","x_test = test_labeled_features[:, :cols]\n","\n","# format features for multi-class classification\n","num_classes = len(label_list)\n","y_train = train_labeled_features[:, cols].astype(int)\n","y_valid = valid_labeled_features[:, cols].astype(int)\n","y_test = test_labeled_features[:, cols].astype(int)\n","\n","print(test_labeled_features[:10])\n","print(y_test[:10])"]},{"cell_type":"markdown","metadata":{"id":"OdxMzCwIrcOL"},"source":["### Training\n","\n","Now, we are going to train an SVM model genre classification using `scikit-learn`. Take the advantage of `CalibratedClassifierCV`, we can easily have the probabilty of each genre that the model predicts."]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14068,"status":"ok","timestamp":1639954115475,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"U7Pc-OdWreRZ","outputId":"43179158-012c-45db-cfa3-f078205b8967"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n","  UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7096645367412141\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]}],"source":["def get_accuracy(y_true, y_pred):\n","    return np.sum(np.equal(y_true, y_pred)) / len(y_true);\n","\n","def train_model(x_train, y_train, x_valid, y_valid):\n","    clf_svm = LinearSVC()\n","    clf = CalibratedClassifierCV(clf_svm)   # outputs the probability\n","    clf.fit(x_train, y_train)\n","    y_pred = clf.predict(x_valid)\n","    print(\"Accuracy on validation dataset:\", get_accuracy(y_valid, y_pred))\n","    return clf\n","\n","classifier = train_model(x_train, y_train, x_valid, y_valid)\n","\n","# svm = LinearSVC()\n","# clf = CalibratedClassifierCV(svm) \n","# clf.fit(x_train, y_train)\n","# y_proba = clf.predict_proba(x_valid)"]},{"cell_type":"markdown","metadata":{"id":"OJcKen3UrmW3"},"source":["### Evaluation\n","\n","Evaluate the model using test dataset."]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1639954182124,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"8IUL6ekCtsHI","outputId":"dbb07ad9-8637-40ac-fe0e-a40e07ae4326"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test dataset: 0.7345309381237525\n"]}],"source":["y_pred = classifier.predict(x_test)\n","print(\"Accuracy on test dataset:\", get_accuracy(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"2sW1a-w64hx7"},"source":["Make a prediction on a specific midi file."]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1639955840913,"user":{"displayName":"Zhiying Cui","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00010876004313184189"},"user_tz":300},"id":"kys-Zk234jdN","outputId":"9cc9eae8-bac2-4b6e-cde2-b591ed11bd54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pop_Rock\n","Predict index: [0]\n","Predict probability: [[0.74334592 0.08222873 0.00355892 0.03770512 0.02307281 0.01321319\n","  0.01109741 0.02731219 0.02011482 0.00104432 0.02143032 0.0049173\n","  0.01095896]]\n"]}],"source":["def make_prediction_prob(clf, midi_path, label_list=label_list):\n","    x = get_features(midi_path)\n","    y_pred = clf.predict([x])\n","    y_pred_prob = clf.predict_proba([x])\n","    index = np.argmax(y_pred[0])\n","    label = label_list[y_pred[0]]\n","    return label, y_pred, y_pred_prob\n","    \n","# Make a Prediction\n","test_midi_path = \"/content/drive/MyDrive/DLFinalProject/lmd_matched/L/L/Y/TRLLYMC128F146EB42/198c242dad2a442219463683abe602fd.mid\"\n","label, y_pred, y_pred_prob = make_prediction_prob(classifier, test_midi_path)\n","print(label)\n","print(\"Predict index:\", y_pred)\n","print(\"Predict probability:\", y_pred_prob)"]},{"cell_type":"markdown","metadata":{"id":"UR3FMjmuaDJl"},"source":["## Summary\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"main-colab.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"py_36","language":"python","name":"py_36"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}
