{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for the Auto-Generated Music Composition\n",
    "\n",
    "Name: Jianqiao Li, Zhiying Cui\n",
    "\n",
    "NetID: jl7136, zc2191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- This project is based on the DeepJ model from the [github repository](https://github.com/calclavia/DeepJ).\n",
    "- The reference paper is [Mao HH, Shin T, Cottrell G. DeepJ: Style-specific music generation. In2018 IEEE 12th International Conference on Semantic Computing (ICSC) 2018 Jan 31 (pp. 377-382). IEEE.](https://arxiv.org/abs/1801.00887)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of This Work\n",
    "Our model aims to auto-generate a 10 to 30 seconds polyphony given a speciﬁc style and random formatted inputs. Polyphony is the abbreviation of single voice polyphony. It is a sequence of notes for a single instrument, where more than one note can be played at the same time.\n",
    "- Rebuild the DeepJ model on our local environment and compared the results with the authors’ model.\n",
    "- Replace the one-hot encoding strategy on style generation with pre-trained model.\n",
    "- Try to develop a sparse representation of music as the authors recommend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Environment\n",
    "\n",
    "- Python version: Python 3.6\n",
    "    - Consider the imcompatable version of `grpcio` in Python 3.5 for the tensorflow, we decide to use Python 3.6\n",
    "- Framework: tensorflow\n",
    "- Environment: Google Cloud\n",
    "- Set up a jupyter server: [Running Jupyter Notebook on Google Cloud Platform in 15 min](https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52)\n",
    "- Set up a different version of kernel\n",
    "    - [How to add python 3.6 kernel alongside 3.5 on jupyter](https://stackoverflow.com/questions/43759610/how-to-add-python-3-6-kernel-alongside-3-5-on-jupyter)\n",
    "    - [Jupyter Notebook Kernels: How to Add, Change, Remove](https://queirozf.com/entries/jupyter-kernels-how-to-add-change-remove)\n",
    "    - [Run Jupyter Notebook script from terminal](https://deeplearning.lipingyang.org/2018/03/28/run-jupyter-notebook-script-from-terminal/)\n",
    "- Install dependencies for the DeepJ\n",
    "```\n",
    "pip install --ignore-installed -r requirements.txt\n",
    "```\n",
    "- Set up python-midi\n",
    "    - The original [python-midi](https://github.com/vishnubob/python-midi) is no longer maintained. We have to find an alternative python-midi which is not only compatible with Python 3 but also suitable to the DeepJ model\n",
    "    - ✅ Candidate 1: https://github.com/louisabraham/python3-midi\n",
    "    - ❓ Candidate 2: https://github.com/sniperwrb/python-midi\n",
    "    - ❓ Candidate 3: https://github.com/jameswenzel/mydy\n",
    "- Download the dataset to `data/` folder\n",
    "    - Piano-Midi: http://www.piano-midi.de/\n",
    "- Directory of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print('Project Directory')\n",
    "os.chdir('/home/choi/DLProject/')\n",
    "!tree -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the DeepJ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Lambda, Reshape, Permute\n",
    "from keras.layers import TimeDistributed, RepeatVector, Conv1D, Activation\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers.merge import Concatenate, Add\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras import losses\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "import argparse\n",
    "import midi\n",
    "import os\n",
    "\n",
    "from dataset import *              # load dataset\n",
    "from generate import *             # generate music\n",
    "from midi_util import midi_encode\n",
    "from model import *                # utils used in building models\n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "Consider the computing expense, we eliminated the genre \"baroque\".\n",
    "- A summarized dataset for [Piano-Midi](http://www.piano-midi.de/): https://www.kaggle.com/soumikrakshit/classical-music-midi\n",
    "- Some other source of dataset\n",
    "    - https://www.mfiles.co.uk/\n",
    "    - https://www.kaggle.com/programgeek01/anime-music-midi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constant parameters\n",
    "\"\"\"\n",
    "\n",
    "# Define the musical styles\n",
    "genre = [\n",
    "    'classical',\n",
    "    'romantic'\n",
    "]\n",
    "\n",
    "styles = [\n",
    "    [\n",
    "        'data/classical/beethoven',\n",
    "        'data/classical/haydn',\n",
    "        'data/classical/mozart'\n",
    "    ],\n",
    "    [\n",
    "        'data/romantic/borodin',\n",
    "        'data/romantic/brahms',\n",
    "        'data/romantic/tschai'\n",
    "    ]\n",
    "]\n",
    "\n",
    "NUM_STYLES = sum(len(s) for s in styles)\n",
    "\n",
    "# MIDI Resolution\n",
    "DEFAULT_RES = 96\n",
    "MIDI_MAX_NOTES = 128  # 1 - 128\n",
    "MAX_VELOCITY = 127    # 0 - 127\n",
    "\n",
    "# Number of octaves supported\n",
    "NUM_OCTAVES = 4\n",
    "OCTAVE = 12\n",
    "\n",
    "# Min and max note (in MIDI note number)\n",
    "MIN_NOTE = 36\n",
    "MAX_NOTE = MIN_NOTE + NUM_OCTAVES * OCTAVE\n",
    "NUM_NOTES = MAX_NOTE - MIN_NOTE\n",
    "\n",
    "# Number of beats in a bar\n",
    "BEATS_PER_BAR = 4\n",
    "# Notes per quarter note\n",
    "NOTES_PER_BEAT = 4\n",
    "# The quickest note is a half-note\n",
    "NOTES_PER_BAR = NOTES_PER_BEAT * BEATS_PER_BAR\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 8 * NOTES_PER_BAR\n",
    "\n",
    "# Hyper Parameters\n",
    "OCTAVE_UNITS = 64\n",
    "STYLE_UNITS = 64\n",
    "NOTE_UNITS = 3\n",
    "TIME_AXIS_UNITS = 256\n",
    "NOTE_AXIS_UNITS = 128\n",
    "\n",
    "TIME_AXIS_LAYERS = 2\n",
    "NOTE_AXIS_LAYERS = 2\n",
    "\n",
    "# Move file save location\n",
    "OUT_DIR = 'out'\n",
    "MODEL_DIR = os.path.join(OUT_DIR, 'models')\n",
    "MODEL_FILE = os.path.join(OUT_DIR, 'model.h5')\n",
    "SAMPLES_DIR = os.path.join(OUT_DIR, 'samples')\n",
    "CACHE_DIR = os.path.join(OUT_DIR, 'cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_models(time_steps=SEQ_LEN, input_dropout=0.2, dropout=0.5):\n",
    "    \"\"\"\n",
    "    Build the LSTM model\n",
    "    \"\"\"\n",
    "    notes_in = Input((time_steps, NUM_NOTES, NOTE_UNITS))\n",
    "    beat_in = Input((time_steps, NOTES_PER_BAR))\n",
    "    style_in = Input((time_steps, NUM_STYLES))\n",
    "    # Target input for conditioning\n",
    "    chosen_in = Input((time_steps, NUM_NOTES, NOTE_UNITS))\n",
    "\n",
    "    # Dropout inputs\n",
    "    notes = Dropout(input_dropout)(notes_in)\n",
    "    beat = Dropout(input_dropout)(beat_in)\n",
    "    chosen = Dropout(input_dropout)(chosen_in)\n",
    "\n",
    "    # Distributed representations\n",
    "    style_l = Dense(STYLE_UNITS, name='style')\n",
    "    style = style_l(style_in)\n",
    "\n",
    "    \"\"\" Time axis \"\"\"\n",
    "    time_out = time_axis(dropout)(notes, beat, style)\n",
    "\n",
    "    \"\"\" Note Axis & Prediction Layer \"\"\"\n",
    "    naxis = note_axis(dropout)\n",
    "    notes_out = naxis(time_out, chosen, style)\n",
    "\n",
    "    model = Model([notes_in, chosen_in, beat_in, style_in], [notes_out])\n",
    "    model.compile(optimizer='nadam', loss=[primary_loss])\n",
    "\n",
    "    \"\"\" Generation Models \"\"\"\n",
    "    time_model = Model([notes_in, beat_in, style_in], [time_out])\n",
    "\n",
    "    note_features = Input((1, NUM_NOTES, TIME_AXIS_UNITS), name='note_features')\n",
    "    chosen_gen_in = Input((1, NUM_NOTES, NOTE_UNITS), name='chosen_gen_in')\n",
    "    style_gen_in = Input((1, NUM_STYLES), name='style_in')\n",
    "\n",
    "    # Dropout inputs\n",
    "    chosen_gen = Dropout(input_dropout)(chosen_gen_in)\n",
    "    style_gen = style_l(style_gen_in)\n",
    "\n",
    "    note_gen_out = naxis(note_features, chosen_gen, style_gen)\n",
    "\n",
    "    note_model = Model([note_features, chosen_gen_in, style_gen_in], note_gen_out)\n",
    "\n",
    "    return model, time_model, note_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(models):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    print('Loading data')\n",
    "    train_data, train_labels = load_all(styles, BATCH_SIZE, SEQ_LEN)\n",
    "\n",
    "    cbs = [\n",
    "        ModelCheckpoint(MODEL_FILE, monitor='loss', save_best_only=True, save_weights_only=True),\n",
    "        EarlyStopping(monitor='loss', patience=5),\n",
    "        TensorBoard(log_dir='out/logs', histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    print('Training')\n",
    "    models[0].fit(train_data, train_labels, epochs=2, callbacks=cbs, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = build_models()\n",
    "models[0].summary()\n",
    "models[1].summary() # time_model\n",
    "models[2].summary() # note_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads all MIDI files as a piano roll. Prepare dataset\n",
    "(For Keras)\n",
    "\"\"\"\n",
    "\n",
    "time_steps = SEQ_LEN\n",
    "note_data = []\n",
    "beat_data = []\n",
    "style_data = []\n",
    "\n",
    "note_target = []\n",
    "\n",
    "# TODO: Can speed this up with better parallel loading. Order gaurentee.\n",
    "stylesEnum = [y for x in styles for y in x]\n",
    "\n",
    "for style_id, style in enumerate(stylesEnum):\n",
    "    style_hot = one_hot(style_id, NUM_STYLES)\n",
    "    # Parallel process all files into a list of music sequences\n",
    "    seqs = Parallel(n_jobs=multiprocessing.cpu_count(), backend='threading')(delayed(load_midi)(f) for f in get_all_files([style]))\n",
    "\n",
    "    for seq in seqs:\n",
    "        if len(seq) >= time_steps:\n",
    "            # Clamp MIDI to note range\n",
    "            seq = clamp_midi(seq)\n",
    "            # Create training data and labels\n",
    "            train_data, label_data = stagger(seq, time_steps)\n",
    "            note_data += train_data\n",
    "            note_target += label_data\n",
    "\n",
    "            beats = [compute_beat(i, NOTES_PER_BAR) for i in range(len(seq))]\n",
    "            beat_data += stagger(beats, time_steps)[0]\n",
    "\n",
    "            style_data += stagger([style_hot for i in range(len(seq))], time_steps)[0]\n",
    "\n",
    "note_data = np.array(note_data)\n",
    "beat_data = np.array(beat_data)\n",
    "style_data = np.array(style_data)\n",
    "note_target = np.array(note_target)\n",
    "\n",
    "train_data = [note_data, note_target, beat_data, style_data]\n",
    "train_labels = [note_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"note_data:\", train_data[0].shape)\n",
    "print(\"beat_data:\", train_data[1].shape)\n",
    "print(\"style_data:\", train_data[2].shape)\n",
    "print(\"note_target:\", train_data[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cbs = [\n",
    "    ModelCheckpoint(MODEL_FILE, monitor='loss', save_best_only=True, save_weights_only=True),\n",
    "    EarlyStopping(monitor='loss', patience=3),\n",
    "    TensorBoard(log_dir='out/logs', histogram_freq=1)\n",
    "]\n",
    "\n",
    "models[0].fit(train_data, train_labels, epochs=1000, callbacks=cbs, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models[0].load_weights(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Generates music.')\n",
    "# parser.add_argument('--bars', default=32, type=int, help='Number of bars to generate')\n",
    "# parser.add_argument('--styles', default=None, type=int, nargs='+', help='Styles to mix together')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "models = build_or_load()\n",
    "\n",
    "stylesGene = [compute_genre(i) for i in range(len(genre))]\n",
    "\n",
    "# if args.styles:\n",
    "#     # Custom style\n",
    "#     styles = [np.mean([one_hot(i, NUM_STYLES) for i in args.styles], axis=0)]\n",
    "\n",
    "write_file('output', generate(models, 32, stylesGene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some thoughts\n",
    "- 2 gnere: classic, jazz, EDM?\n",
    "- create a table to present constant value\n",
    "- make a repository on github\n",
    "- music classification \n",
    "    - CNN wave images\n",
    "    - https://www.analyticsvidhya.com/blog/2021/06/music-genres-classification-using-deep-learning-techniques/\n",
    "    - http://cs229.stanford.edu/proj2018/report/21.pdf\n",
    "    - Midi\n",
    "    - https://github.com/sandershihacker/midi-classification-tutorial/blob/master/midi_classifier.ipynb\n",
    "    - ByteDance https://arxiv.org/abs/2010.14805# \n",
    "    - ByteDance dataset https://arxiv.org/abs/2010.07061 Github https://github.com/bytedance/GiantMIDI-Piano\n",
    "    \n",
    "### what is the next steps\n",
    "- change the DeepJ model to adapt more genre ranther than one specific composer?\n",
    "- or pre-train a music classification model to initialize the input -> change one-hot representation\n",
    "- forget about the sparse input, enough explaination is ok\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36",
   "language": "python",
   "name": "py_36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
